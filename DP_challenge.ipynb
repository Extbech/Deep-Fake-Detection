{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Fake Detection Challenge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import Tuple\n",
    "from model2D import *\n",
    "#from model3D import *\n",
    "from model3D_small import *\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.utils.np_utils  import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib import offsetbox\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import visualkeras\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Patch\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videoes = \"data/train_sample_videos\"\n",
    "test_videos = \"data/test_videos\"\n",
    "\n",
    "print(f\"Train Videoes: {len(os.listdir(train_videoes))}\\nTest Vidoes: {len(os.listdir(test_videos))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_json(train_videoes+\"/metadata.json\").T\n",
    "labels_col =meta_data[\"label\"].to_list()\n",
    "paths_col = meta_data.index.to_list()\n",
    "print(meta_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_real_vs_fake(class_count: list) -> None:\n",
    "    plt.pie(class_count, labels=[\"Real Videos\", \"Deep Fake Videos\"], autopct='%.2f%%',\n",
    "       wedgeprops={'linewidth': 2.5, 'edgecolor': 'white'},\n",
    "       textprops={'size': 'large', 'fontweight': 'bold'})\n",
    "    plt.title(\"Proportion of Real vs Deep Fake videos in the training dataset.\", fontdict={'fontweight': 'bold'})\n",
    "    plt.legend([f\"Real Videos Count: {class_count[0]}\", f\"Deep Fake Videos Count: {class_count[1]}\"], bbox_to_anchor=(0.5, 0.05), bbox_transform=plt.gcf().transFigure, loc=\"lower center\", prop={'weight':'bold'})\n",
    "    plt.savefig(\"images/pie_chart_class_proportions.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_series = meta_data[\"label\"].value_counts()\n",
    "fake_count = label_count_series[\"FAKE\"]\n",
    "real_count = label_count_series[\"REAL\"]\n",
    "\n",
    "visualize_real_vs_fake([real_count, fake_count])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite skewed dataset. Might want to consider upsampling of real classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_v1(paths: list, frames_each_video: int, video_amount: int) -> list:\n",
    "    video_array_colors = []\n",
    "    for idx, path in enumerate(paths): \n",
    "        if idx == video_amount:\n",
    "            break\n",
    "        vc = cv2.VideoCapture(path)\n",
    "        frames_to_skip = (int(vc.get(cv2.CAP_PROP_FRAME_COUNT))-5)/frames_each_video\n",
    "        frames_to_skip = math.floor(frames_to_skip)\n",
    "        video = []\n",
    "        i = 0\n",
    "        while vc.isOpened():\n",
    "            i += 1\n",
    "            ret, frame = vc.read()\n",
    "            if ret and frame is not None:\n",
    "                if i % frames_to_skip != 0:\n",
    "                    continue\n",
    "                if frame.shape[0] == 1920:\n",
    "                    frame = frame.transpose(1, 0, 2)\n",
    "                frame = cv2.resize(frame, (1080, 720))\n",
    "                video.append((cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255))\n",
    "            else:\n",
    "                vc.release()\n",
    "                break\n",
    "        #if len(video) < 150:        # for Ã¥ catch vid me for lite frames\n",
    "        #    print(len(video), idx)\n",
    "        #    video.append(video[-1])\n",
    "        #    print(len(video))\n",
    "        video_array_colors.append(np.array(video))\n",
    "    return np.array(video_array_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_frames_v2(paths: list, frames_each_video: int, video_amount: int):\n",
    "    video_array_colors = []\n",
    "    face_regions = []\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    for idx, path in enumerate(paths): \n",
    "        if idx == video_amount:\n",
    "            break\n",
    "\n",
    "        vc = cv2.VideoCapture(path)\n",
    "        frames_to_skip = (int(vc.get(cv2.CAP_PROP_FRAME_COUNT))-5)/frames_each_video\n",
    "        frames_to_skip = math.floor(frames_to_skip)\n",
    "        video = []\n",
    "        i = 0\n",
    "\n",
    "        while vc.isOpened():\n",
    "            i += 1\n",
    "            ret, frame = vc.read()\n",
    "            if ret and frame is not None:\n",
    "                if i % frames_to_skip != 0:\n",
    "                    continue\n",
    "                frame = cv2.resize(frame, (1080, 720))\n",
    "                video.append(frame)\n",
    "            else:\n",
    "                vc.release()\n",
    "                break\n",
    "\n",
    "        video_array_colors.append(np.array(video))\n",
    "\n",
    "        video_face_regions = []\n",
    "\n",
    "        for frame in video:\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = max(faces, key=lambda face: face[2] * face[3])\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "                video_face_regions.append(face_img)\n",
    "\n",
    "        if video_face_regions:\n",
    "            face_regions.append(video_face_regions[0])\n",
    "        else:\n",
    "            print(\"No faces detected in any frame of this video.\")\n",
    "            face_regions.append(np.zeros((224, 224, 3)))  # Add a placeholder image with the same size as the face images\n",
    "\n",
    "    return np.array(video_array_colors), face_regions\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(face_regions, model):\n",
    "    features = []\n",
    "    for face_img in face_regions:\n",
    "        if np.count_nonzero(face_img) == 0:  # If the face image is a placeholder (all zeros)\n",
    "            features.append(np.zeros_like(features[-1]))  # Add zeros as features\n",
    "        else:\n",
    "            face_img = cv2.resize(face_img, (224, 224))\n",
    "            face_img = preprocess_input(face_img)\n",
    "            face_img = np.expand_dims(face_img, axis=0)\n",
    "            feature = model.predict(face_img)\n",
    "            features.append(feature.squeeze())\n",
    "    return features\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "video_array_colors, face_regions = get_frames_v2(paths=complete_paths, frames_each_video=5, video_amount=200)\n",
    "video_features = extract_features(face_regions, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos_sample = os.listdir(train_videoes)\n",
    "sample_file_names = training_videos_sample.copy()\n",
    "complete_paths = []\n",
    "for path in paths_col:\n",
    "    complete_paths.append(train_videoes+\"/\"+path)\n",
    "complete_paths.sort()\n",
    "vid_arr_col = get_frames_v1(paths=complete_paths, frames_each_video=10, video_amount=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vid_arr_col.shape)\n",
    "#vid_arr_gray = np.expand_dims(vid_arr_gray, axis=-1)\n",
    "#print(vid_arr_gray.shape)\n",
    "print(vid_arr_col[8].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0 if val!=\"FAKE\" else 1 for val in labels_col ]\n",
    "y =np.array(y)\n",
    "y = to_categorical(y, num_classes=None).astype(int)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of Videos: {len(vid_arr_col)}\")\n",
    "# print(f\"Frames for videos: {[len(vid_arr_col[i]) for i in range(len(vid_arr_col))]}\")\n",
    "print(f\"Frames for videos: {[len(vid_arr_col[i]) for i in range(10)]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(video_array):\n",
    "    features = []\n",
    "    face_regions = []\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    noFaceFound = 0\n",
    "    for video in tqdm(video_array):\n",
    "        video_features = []\n",
    "        video_face_regions = []\n",
    "\n",
    "        for frame in video:\n",
    "            gray_frame = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = max(faces, key=lambda face: face[2] * face[3])\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "                face_img = cv2.resize(face_img, (224, 224))\n",
    "\n",
    "                face_img = preprocess_input(face_img * 255)\n",
    "                face_img = np.expand_dims(face_img, axis=0)\n",
    "                feature = model.predict(face_img)\n",
    "                video_features.append(feature.squeeze())\n",
    "\n",
    "                video_face_regions.append(frame[y:y+h, x:x+w])\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if video_features:\n",
    "            features.append(np.mean(video_features, axis=0))\n",
    "            face_regions.append(video_face_regions[0])\n",
    "        else:\n",
    "            print(\"No faces detected in any frame of this video.\")\n",
    "            noFaceFound += 1\n",
    "            if features:  # Check if the features list is not empty\n",
    "                features.append(np.zeros_like(features[-1]))  # Add zeros if no faces are detected\n",
    "                face_regions.append(np.zeros((224, 224, 3)))  # Add a placeholder image with the same size as the face images\n",
    "\n",
    "    return np.array(features), face_regions\n",
    "video_features, face_regions = extract_features(vid_arr_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_with_images(tsne_results, face_regions, figsize=(4, 4), thumbnail_size=(64, 36)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    for i in range(tsne_results.shape[0]):\n",
    "        x, y = tsne_results[i, :]\n",
    "\n",
    "        img = face_regions[i]\n",
    "        img = cv2.resize(img, thumbnail_size)\n",
    "\n",
    "        img_box = offsetbox.OffsetImage(img, zoom=1, cmap='gray')\n",
    "        img_annotation = offsetbox.AnnotationBbox(img_box, (x, y), xycoords='data', frameon=False)\n",
    "\n",
    "        ax.add_artist(img_annotation)\n",
    "\n",
    "    ax.set_xlim(tsne_results[:, 0].min() - 10, tsne_results[:, 0].max() + 10)\n",
    "    ax.set_ylim(tsne_results[:, 1].min() - 10, tsne_results[:, 1].max() + 10)\n",
    "    ax.set_xlabel('t-SNE Component 1')\n",
    "    ax.set_ylabel('t-SNE Component 2')\n",
    "    ax.set_title('t-SNE Plot of Video Features with Face Thumbnails')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42,perplexity=20)\n",
    "tsne_results = tsne.fit_transform(video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vis = visualkeras.layered_view(test_m, to_file=\"images/layers_yo.png\", legend=True)\n",
    "vis.show()\n",
    "plt.rcParams.update({'font.size':500})  # set the legend font size to 56\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_model(test_m, to_file=\"images/layerv2.png\", show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_images(tsne_results, face_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_video(video: list, figsize: tuple, width: int, height: int) -> None:\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(len(video[:(width*height)])):\n",
    "        plt.subplot(width, height, i+1)\n",
    "        plt.imshow(video[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate some example data\n",
    "data = np.random.rand(10, 10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = ax.imshow(data, cmap='viridis')\n",
    "\n",
    "# Create the colorbar\n",
    "cbar = fig.colorbar(heatmap, ax=ax)\n",
    "\n",
    "# Change the font size of the colorbar labels\n",
    "cbar.ax.tick_params(labelsize=14)  # You can set the desired font size here\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_faces(faces: list, figsize: tuple, width: int, height: int) -> None:\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    num_faces = min(len(faces), width * height)\n",
    "    for i in range(num_faces):\n",
    "        plt.subplot(width, height, i + 1)\n",
    "        plt.imshow(faces[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_faces(face_regions, figsize=(10, 10), width=5, height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_video(video_features[0], (30,5), 2, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD MODEL\n",
    "- Add Layers\n",
    "- Add Loss function, optimizers, and metrics\n",
    "- Compile model and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_class = compute_class_weight(class_weight='balanced',classes=[0,1],y=np.argmax(y, axis=1))\n",
    "class_weights = dict(zip(np.unique(y), weight_class))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3D_model(input_data):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Conv3D(filters=64, kernel_size=3, padding=\"same\", strides=1, activation=\"relu\", input_shape=input_data.shape[1:]))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=8, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(2 ,activation=\"softmax\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(input_data):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Conv3D(filters=32, kernel_size=(2,2,2),input_shape=input_data.shape[1:],\n",
    "                activation='relu',\n",
    "                padding='same', data_format='channels_last'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ConvLSTM2D(filters=16, kernel_size=(2, 2),\n",
    "                    padding='same', return_sequences=True))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "    model.add(layers.ConvLSTM2D(filters=16, kernel_size=(2, 2),\n",
    "                    padding='same', return_sequences=True))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2,2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dense(32,activation='elu'))\n",
    "\n",
    "    model.add(layers.Dense(2,activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m = test_model(vid_arr_col)\n",
    "test_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "optimizer = \"adam\"\n",
    "metrics=[\"accuracy\"]\n",
    "test_m.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_test = test_m.fit(vid_arr_col, \n",
    "                          y[:vid_arr_col.shape[0]],\n",
    "                          epochs=10, \n",
    "                          batch_size=10, \n",
    "                          verbose=1, \n",
    "                          class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = test_m.predict(vid_arr_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y = np.argmax(y[:vid_arr_col.shape[0]], axis=1)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "print(classification_report(actual_y, pred_y, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(actual_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfm, display_labels= [\"REAL\", \"FAKE\"])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3D_col = build_3D_model(vid_arr_col)\n",
    "model_3D_col.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.1)\n",
    "metrics=[\"accuracy\"]\n",
    "model_3D_col.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_3D_col.fit(vid_arr_col, \n",
    "                           y[:vid_arr_col.shape[0]],\n",
    "                           epochs=10, \n",
    "                           batch_size=10, \n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "metrics=[\"accuracy\"]\n",
    "model_3D_col.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_test.params)\n",
    "print(history_test.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_test.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_test.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
