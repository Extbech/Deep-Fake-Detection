{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Fake Detection Challenge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import Tuple\n",
    "from model2D import *\n",
    "#from model3D import *\n",
    "from model3D_small import *\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.utils.np_utils  import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib import offsetbox\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import visualkeras\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Patch\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videoes = \"data/train_sample_videos\"\n",
    "test_videos = \"data/test_videos\"\n",
    "\n",
    "print(f\"Train Videoes: {len(os.listdir(train_videoes))}\\nTest Vidoes: {len(os.listdir(test_videos))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_json(train_videoes+\"/metadata.json\").T\n",
    "labels_col =meta_data[\"label\"].to_list()\n",
    "paths_col = meta_data.index.to_list()\n",
    "print(meta_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_real_vs_fake(class_count: list) -> None:\n",
    "    plt.pie(class_count, labels=[\"Real Videos\", \"Deep Fake Videos\"], autopct='%.2f%%',\n",
    "       wedgeprops={'linewidth': 2.5, 'edgecolor': 'white'},\n",
    "       textprops={'size': 'large', 'fontweight': 'bold'})\n",
    "    plt.title(\"Proportion of Real vs Deep Fake videos in the training dataset.\", fontdict={'fontweight': 'bold'})\n",
    "    plt.legend([f\"Real Videos Count: {class_count[0]}\", f\"Deep Fake Videos Count: {class_count[1]}\"], bbox_to_anchor=(0.5, 0.05), bbox_transform=plt.gcf().transFigure, loc=\"lower center\", prop={'weight':'bold'})\n",
    "    plt.savefig(\"images/pie_chart_class_proportions.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_series = meta_data[\"label\"].value_counts()\n",
    "fake_count = label_count_series[\"FAKE\"]\n",
    "real_count = label_count_series[\"REAL\"]\n",
    "\n",
    "visualize_real_vs_fake([real_count, fake_count])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite skewed dataset. Might want to consider upsampling of real classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces(paths: list, face_amount: int) -> (np.ndarray, list):\n",
    "    video_array = []\n",
    "    invalid_indices = []\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    #model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    for idx, path in enumerate(tqdm(paths)):\n",
    "        vc = cv2.VideoCapture(path)\n",
    "\n",
    "        faces = []\n",
    "        while len(faces) < face_amount:\n",
    "            ret, frame = vc.read()\n",
    "            if ret and frame is not None:    \n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                face = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "                if len(face) > 0:\n",
    "                    x, y, w, h = max(face, key=lambda x: x[2] * x[3])\n",
    "                    face_img = frame[y:y+h, x:x+w]\n",
    "                    face_img = cv2.resize(face_img, (224, 224))\n",
    "                    #face_img = np.expand_dims(face_img, axis=0)\n",
    "                    #feature = model.predict(face_img)\n",
    "                    faces.append(face_img)\n",
    "            else:\n",
    "                break\n",
    "        vc.release()\n",
    "        if len(faces) == face_amount:\n",
    "            video_array.append(np.array(faces))\n",
    "        else:\n",
    "            invalid_indices.append(idx)\n",
    "\n",
    "    \n",
    "    \n",
    "    return np.array(video_array), invalid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos_sample = os.listdir(train_videoes)\n",
    "sample_file_names = training_videos_sample.copy()\n",
    "complete_paths = []\n",
    "\n",
    "for path in paths_col:\n",
    "    complete_paths.append(train_videoes+\"/\"+path)\n",
    "complete_paths.sort()\n",
    "\n",
    "(faces, indices) = get_faces(complete_paths, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faces[0][1].shape)\n",
    "print(faces[1][1].shape)\n",
    "print(faces.shape)\n",
    "print(indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_v1(paths: list, frames_each_video: int, video_amount: int, resolution: tuple) -> list:\n",
    "    video_array_colors = []\n",
    "    for idx, path in enumerate(paths): \n",
    "        if idx == video_amount:\n",
    "            break\n",
    "        vc = cv2.VideoCapture(path)\n",
    "\n",
    "        frames_to_skip = (int(vc.get(cv2.CAP_PROP_FRAME_COUNT))-5)/frames_each_video\n",
    "        frames_to_skip = int(round(frames_to_skip,0))\n",
    "\n",
    "        video = []\n",
    "        i = 0\n",
    "        while vc.isOpened():\n",
    "            i += 1\n",
    "            ret, frame = vc.read()\n",
    "            if ret and frame is not None:\n",
    "                if i % frames_to_skip != 0:\n",
    "                    continue\n",
    "                if frame.shape[0] == 1920:\n",
    "                    frame = frame.transpose(1, 0, 2)\n",
    "                    \n",
    "                frame = cv2.resize(frame, resolution)\n",
    "                video.append((cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255))\n",
    "            else:\n",
    "                vc.release()\n",
    "                break\n",
    "            if len(video) < frames_each_video:        # for Ã¥ catch vid me for lite frames\n",
    "                video.append(video[-1])\n",
    "        video_array_colors.append(np.array(video))\n",
    "    return np.array(video_array_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_frames_v2(paths: list, frames_each_video: int, video_amount: int):\n",
    "    video_array_colors = []\n",
    "    face_regions = []\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    for idx, path in enumerate(paths): \n",
    "        if idx == video_amount:\n",
    "            break\n",
    "\n",
    "        vc = cv2.VideoCapture(path)\n",
    "        frames_to_skip = (int(vc.get(cv2.CAP_PROP_FRAME_COUNT))-5)/frames_each_video\n",
    "        frames_to_skip = math.floor(frames_to_skip)\n",
    "        video = []\n",
    "        i = 0\n",
    "\n",
    "        while vc.isOpened():\n",
    "            i += 1\n",
    "            ret, frame = vc.read()\n",
    "            if ret and frame is not None:\n",
    "                if i % frames_to_skip != 0:\n",
    "                    continue\n",
    "                frame = cv2.resize(frame, (1080, 720))\n",
    "                video.append(frame)\n",
    "            else:\n",
    "                vc.release()\n",
    "                break\n",
    "\n",
    "        video_array_colors.append(np.array(video))\n",
    "\n",
    "        video_face_regions = []\n",
    "\n",
    "        for frame in video:\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = max(faces, key=lambda face: face[2] * face[3])\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "                video_face_regions.append(face_img)\n",
    "\n",
    "        if video_face_regions:\n",
    "            face_regions.append(video_face_regions[0])\n",
    "        else:\n",
    "            print(\"No faces detected in any frame of this video.\")\n",
    "            face_regions.append(np.zeros((224, 224, 3)))  # Add a placeholder image with the same size as the face images\n",
    "\n",
    "    return np.array(video_array_colors), face_regions\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(face_regions, model):\n",
    "    features = []\n",
    "    for face_img in face_regions:\n",
    "        if np.count_nonzero(face_img) == 0:  # If the face image is a placeholder (all zeros)\n",
    "            features.append(np.zeros_like(features[-1]))  # Add zeros as features\n",
    "        else:\n",
    "            face_img = cv2.resize(face_img, (224, 224))\n",
    "            face_img = preprocess_input(face_img)\n",
    "            face_img = np.expand_dims(face_img, axis=0)\n",
    "            feature = model.predict(face_img)\n",
    "            features.append(feature.squeeze())\n",
    "    return features\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "video_array_colors, face_regions = get_frames_v2(paths=complete_paths, frames_each_video=5, video_amount=200)\n",
    "video_features = extract_features(face_regions, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos_sample = os.listdir(train_videoes)\n",
    "sample_file_names = training_videos_sample.copy()\n",
    "complete_paths = []\n",
    "\n",
    "for path in paths_col:\n",
    "    complete_paths.append(train_videoes+\"/\"+path)\n",
    "complete_paths.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vid_arr_col.shape)\n",
    "print(vid_arr_col[8].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0 if val!=\"FAKE\" else 1 for val in labels_col ]\n",
    "y =np.array(y)\n",
    "y = to_categorical(y, num_classes=None).astype(int)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of Videos: {len(vid_arr_col)}\")\n",
    "# print(f\"Frames for videos: {[len(vid_arr_col[i]) for i in range(len(vid_arr_col))]}\")\n",
    "print(f\"Frames for videos: {[len(vid_arr_col[i]) for i in range(10)]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(video_array):\n",
    "    features = []\n",
    "    face_regions = []\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    noFaceFound = 0\n",
    "    for video in tqdm(video_array):\n",
    "        video_features = []\n",
    "        video_face_regions = []\n",
    "\n",
    "        for frame in video:\n",
    "            gray_frame = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            if len(faces) > 0:\n",
    "                x, y, w, h = max(faces, key=lambda face: face[2] * face[3])\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "                face_img = cv2.resize(face_img, (224, 224))\n",
    "\n",
    "                face_img = preprocess_input(face_img * 255)\n",
    "                face_img = np.expand_dims(face_img, axis=0)\n",
    "                feature = model.predict(face_img)\n",
    "                video_features.append(feature.squeeze())\n",
    "\n",
    "                video_face_regions.append(frame[y:y+h, x:x+w])\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if video_features:\n",
    "            features.append(np.mean(video_features, axis=0))\n",
    "            face_regions.append(video_face_regions[0])\n",
    "        else:\n",
    "            print(\"No faces detected in any frame of this video.\")\n",
    "            noFaceFound += 1\n",
    "            if features:  # Check if the features list is not empty\n",
    "                features.append(np.zeros_like(features[-1]))  # Add zeros if no faces are detected\n",
    "                face_regions.append(np.zeros((224, 224, 3)))  # Add a placeholder image with the same size as the face images\n",
    "\n",
    "    return np.array(features), face_regions\n",
    "video_features, face_regions = extract_features(vid_arr_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_with_images(tsne_results, face_regions, figsize=(4, 4), thumbnail_size=(64, 36)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    for i in range(tsne_results.shape[0]):\n",
    "        x, y = tsne_results[i, :]\n",
    "\n",
    "        img = face_regions[i]\n",
    "        img = cv2.resize(img, thumbnail_size)\n",
    "\n",
    "        img_box = offsetbox.OffsetImage(img, zoom=1, cmap='gray')\n",
    "        img_annotation = offsetbox.AnnotationBbox(img_box, (x, y), xycoords='data', frameon=False)\n",
    "\n",
    "        ax.add_artist(img_annotation)\n",
    "\n",
    "    ax.set_xlim(tsne_results[:, 0].min() - 10, tsne_results[:, 0].max() + 10)\n",
    "    ax.set_ylim(tsne_results[:, 1].min() - 10, tsne_results[:, 1].max() + 10)\n",
    "    ax.set_xlabel('t-SNE Component 1')\n",
    "    ax.set_ylabel('t-SNE Component 2')\n",
    "    ax.set_title('t-SNE Plot of Video Features with Face Thumbnails')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42,perplexity=20)\n",
    "tsne_results = tsne.fit_transform(video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vis = visualkeras.layered_view(test_m, to_file=\"images/layers_yo.png\", legend=True)\n",
    "vis.show()\n",
    "plt.rcParams.update({'font.size':500})  # set the legend font size to 56\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_model(test_m, to_file=\"images/layerv2.png\", show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_images(tsne_results, face_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_video(video: list, figsize: tuple, width: int, height: int) -> None:\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(len(video[:(width*height)])):\n",
    "        plt.subplot(width, height, i+1)\n",
    "        plt.imshow(video[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate some example data\n",
    "data = np.random.rand(10, 10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = ax.imshow(data, cmap='viridis')\n",
    "\n",
    "# Create the colorbar\n",
    "cbar = fig.colorbar(heatmap, ax=ax)\n",
    "\n",
    "# Change the font size of the colorbar labels\n",
    "cbar.ax.tick_params(labelsize=14)  # You can set the desired font size here\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_faces(faces: list, figsize: tuple, width: int, height: int) -> None:\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    num_faces = min(len(faces), width * height)\n",
    "    for i in range(num_faces):\n",
    "        plt.subplot(width, height, i + 1)\n",
    "        plt.imshow(faces[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_faces(face_regions, figsize=(10, 10), width=5, height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_video(video_features[0], (30,5), 2, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD MODEL\n",
    "- Add Layers\n",
    "- Add Loss function, optimizers, and metrics\n",
    "- Compile model and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_class = compute_class_weight(class_weight='balanced',classes=[0,1],y=np.argmax(y, axis=1))\n",
    "class_weights = dict(zip(np.unique(y), weight_class))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3D_model(input_data):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Conv3D(filters=64, kernel_size=3, padding=\"same\", strides=1, activation=\"relu\", input_shape=input_data.shape[1:]))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=8, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(2 ,activation=\"softmax\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(input_data):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Conv3D(filters=32, kernel_size=(2,2,2),input_shape=input_data.shape[1:],\n",
    "                activation='relu',\n",
    "                padding='same', data_format='channels_last'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ConvLSTM2D(filters=16, kernel_size=(2, 2),\n",
    "                    padding='same', return_sequences=True))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "    model.add(layers.ConvLSTM2D(filters=16, kernel_size=(2, 2),\n",
    "                    padding='same', return_sequences=True))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2,2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dense(32,activation='elu'))\n",
    "\n",
    "    model.add(layers.Dense(2,activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m = test_model(vid_arr_col)\n",
    "test_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "optimizer = \"adam\"\n",
    "metrics=[\"accuracy\"]\n",
    "test_m.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_test = test_m.fit(vid_arr_col, \n",
    "                          y[:vid_arr_col.shape[0]],\n",
    "                          epochs=10, \n",
    "                          batch_size=10, \n",
    "                          verbose=1, \n",
    "                          class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = test_m.predict(vid_arr_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y = np.argmax(y[:vid_arr_col.shape[0]], axis=1)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "print(classification_report(actual_y, pred_y, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(actual_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfm, display_labels= [\"REAL\", \"FAKE\"])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3D_col = build_3D_model(vid_arr_col)\n",
    "model_3D_col.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.1)\n",
    "metrics=[\"accuracy\"]\n",
    "model_3D_col.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_3D_col.fit(vid_arr_col, \n",
    "                           y[:vid_arr_col.shape[0]],\n",
    "                           epochs=10, \n",
    "                           batch_size=10, \n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "metrics=[\"accuracy\"]\n",
    "model_3D_col.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_test.params)\n",
    "print(history_test.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_test.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_test.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
