{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Fake Detection Challenge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import Tuple\n",
    "from model2D import *\n",
    "from model3D import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videoes = \"data/train_sample_videos\"\n",
    "test_videos = \"data/test_videos\"\n",
    "\n",
    "print(f\"Train Videoes: {len(os.listdir(train_videoes))}\\nTest Vidoes: {len(os.listdir(test_videos))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_json(train_videoes+\"/metadata.json\").T\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_real_vs_fake(class_count: list) -> None:\n",
    "    plt.pie(class_count, labels=[\"Real Videos\", \"Deep Fake Videos\"], autopct='%.2f%%',\n",
    "       wedgeprops={'linewidth': 2.5, 'edgecolor': 'white'},\n",
    "       textprops={'size': 'large', 'fontweight': 'bold'})\n",
    "    plt.title(\"Proportion of Real vs Deep Fake videos in the training dataset.\", fontdict={'fontweight': 'bold'})\n",
    "    plt.legend([f\"Real Videos Count: {class_count[0]}\", f\"Deep Fake Videos Count: {class_count[1]}\"], bbox_to_anchor=(0.5, 0.05), bbox_transform=plt.gcf().transFigure, loc=\"lower center\", prop={'weight':'bold'})\n",
    "    plt.savefig(\"images/pie_chart_class_proportions.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_series = meta_data[\"label\"].value_counts()\n",
    "fake_count = label_count_series[\"FAKE\"]\n",
    "real_count = label_count_series[\"REAL\"]\n",
    "\n",
    "visualize_real_vs_fake([real_count, fake_count])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite skewed dataset. Might want to consider upsampling of real classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_v1(paths: list, frames_each_video: int, video_amount: int) -> list:\n",
    "    video_array = []\n",
    "    frames_to_skip = 300/frames_each_video\n",
    "    for idx, path in enumerate(paths): # bedre me enumerate her?\n",
    "        if idx == video_amount:\n",
    "            break\n",
    "        vc = cv2.VideoCapture(path)\n",
    "        video = []\n",
    "        i = 0\n",
    "        while vc.isOpened():\n",
    "            i += 1\n",
    "            ret, frame = vc.read()\n",
    "            if ret and frame is not None:\n",
    "                if i % frames_to_skip != 0:\n",
    "                    continue\n",
    "                if frame.shape[0] == 1920:\n",
    "                    frame = frame.transpose(1, 0, 2)\n",
    "                frame = cv2.resize(frame, (1280, 720))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                if frame is None:\n",
    "                    continue\n",
    "                video.append(frame)\n",
    "            else:\n",
    "                vc.release()\n",
    "                break\n",
    "        video_array.append(np.array(video))\n",
    "    return np.array(video_array)\n",
    "\n",
    "training_videos_sample = os.listdir(train_videoes)\n",
    "sample_file_names = training_videos_sample.copy()\n",
    "complete_paths = []\n",
    "for i in range(len(training_videos_sample)):\n",
    "    if training_videos_sample[i].endswith('mp4'):\n",
    "        complete_paths.append(train_videoes+\"/\"+training_videos_sample[i])\n",
    "video_array = get_frames_v1(paths=complete_paths, frames_each_video=2, video_amount=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of Videos: {len(video_array)}\")\n",
    "print(f\"Frames for videos: {[len(video_array[i]) for i in range(len(video_array))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_video(video: list, figsize: tuple, width: int, height: int) -> None:\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(len(video[:(width*height)])):\n",
    "        plt.subplot(width, height, i+1)\n",
    "        plt.imshow(video[i])\n",
    "    plt.show()\n",
    "\n",
    "plot_video(video_array[6], (30,5), 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "frames_list = video_array[0]\n",
    "frames_array = np.array([frame.flatten() for frame in frames_list])\n",
    "# Instantiate t-SNE object with desired parameters\n",
    "print(2)\n",
    "tsne = TSNE(n_components=2, perplexity=10, random_state=0)\n",
    "\n",
    "# Fit t-SNE on the frames array\n",
    "frames_tsne = tsne.fit_transform(frames_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming labels is a list of labels corresponding to each frame\n",
    "plt.scatter(frames_tsne[:,0], frames_tsne[:,1], c=range(0,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(vid_arr):\n",
    "    flatten_first_video_frame = vid_arr[0][0]\n",
    "    flatten_first_video_frame = flatten_first_video_frame / 255\n",
    "    return np.array([flatten_first_video_frame])\n",
    "first_frame_formatted = format_frames(video_array)\n",
    "first_frame_formatted.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD MODEL\n",
    "- Add Layers\n",
    "- Add Loss function, optimizers, and metrics\n",
    "- Compile model and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2D = build_2D_model(first_frame_formatted)\n",
    "model_2D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2D = compile_2D_model(model_2D, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3D = build_3D_model(video_array)\n",
    "model_3D.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
