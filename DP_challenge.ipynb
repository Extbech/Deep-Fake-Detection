{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Fake Detection Challenge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import Tuple\n",
    "from model2D import *\n",
    "#from model3D import *\n",
    "from model3D_small import *\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.utils.np_utils  import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videoes = \"data/train_sample_videos\"\n",
    "test_videos = \"data/test_videos\"\n",
    "\n",
    "print(f\"Train Videoes: {len(os.listdir(train_videoes))}\\nTest Vidoes: {len(os.listdir(test_videos))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_json(train_videoes+\"/metadata.json\").T\n",
    "labels_col =meta_data[\"label\"].to_list()\n",
    "paths_col = meta_data.index.to_list()\n",
    "print(meta_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_real_vs_fake(class_count: list) -> None:\n",
    "    plt.pie(class_count, labels=[\"Real Videos\", \"Deep Fake Videos\"], autopct='%.2f%%',\n",
    "       wedgeprops={'linewidth': 2.5, 'edgecolor': 'white'},\n",
    "       textprops={'size': 'large', 'fontweight': 'bold'})\n",
    "    plt.title(\"Proportion of Real vs Deep Fake videos in the training dataset.\", fontdict={'fontweight': 'bold'})\n",
    "    plt.legend([f\"Real Videos Count: {class_count[0]}\", f\"Deep Fake Videos Count: {class_count[1]}\"], bbox_to_anchor=(0.5, 0.05), bbox_transform=plt.gcf().transFigure, loc=\"lower center\", prop={'weight':'bold'})\n",
    "    plt.savefig(\"images/pie_chart_class_proportions.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_series = meta_data[\"label\"].value_counts()\n",
    "fake_count = label_count_series[\"FAKE\"]\n",
    "real_count = label_count_series[\"REAL\"]\n",
    "\n",
    "visualize_real_vs_fake([real_count, fake_count])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite skewed dataset. Might want to consider upsampling of real classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/kp7mjww52gv0qrxc8_6p5dgh0000gn/T/ipykernel_58089/63456889.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(video_array_colors)\n"
     ]
    }
   ],
   "source": [
    "def get_frames_v1(paths: list, frames_each_video: int, video_amount: int) -> list:\n",
    "    video_array_colors = []\n",
    "    for idx, path in enumerate(paths): # bedre me enumerate her?\n",
    "        if idx == video_amount:\n",
    "            break\n",
    "        vc = cv2.VideoCapture(path)\n",
    "        frames_to_skip = (int(vc.get(cv2.CAP_PROP_FRAME_COUNT))-1)/frames_each_video\n",
    "        frames_to_skip=math.floor(frames_to_skip)\n",
    "        \"\"\" print(int(vc.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "        if(int(vc.get(cv2.CAP_PROP_FRAME_COUNT))!=300):\n",
    "            print(path)\n",
    "            print(int(vc.get(cv2.CAP_PROP_FRAME_COUNT))) \"\"\"\n",
    "        video = []\n",
    "        i = 0\n",
    "        while vc.isOpened():\n",
    "            i += 1\n",
    "            ret, frame = vc.read()\n",
    "            if ret and frame is not None:\n",
    "                if i % frames_to_skip != 0:\n",
    "                    continue\n",
    "                if frame.shape[0] == 1920:\n",
    "                    frame = frame.transpose(1, 0, 2)\n",
    "                frame = cv2.resize(frame, (53, 30))\n",
    "                video.append((cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255))\n",
    "            else:\n",
    "                vc.release()\n",
    "                break\n",
    "        video_array_colors.append(np.array(video))\n",
    "    return np.array(video_array_colors)\n",
    "\n",
    "training_videos_sample = os.listdir(train_videoes)\n",
    "sample_file_names = training_videos_sample.copy()\n",
    "complete_paths = []\n",
    "for path in paths_col:\n",
    "    complete_paths.append(train_videoes+\"/\"+path)\n",
    "complete_paths.sort()\n",
    "vid_arr_col = get_frames_v1(paths=complete_paths, frames_each_video=100, video_amount=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vid_arr_col.shape)\n",
    "#vid_arr_gray = np.expand_dims(vid_arr_gray, axis=-1)\n",
    "#print(vid_arr_gray.shape)\n",
    "print(vid_arr_col[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0 if val!=\"FAKE\" else 1 for val in labels_col ]\n",
    "y =np.array(y)\n",
    "y = to_categorical(y, num_classes=None).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of Videos: {len(vid_arr_col)}\")\n",
    "print(f\"Frames for videos: {[len(vid_arr_col[i]) for i in range(len(vid_arr_col))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_video(video: list, figsize: tuple, width: int, height: int) -> None:\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(len(video[:(width*height)])):\n",
    "        plt.subplot(width, height, i+1)\n",
    "        plt.imshow(video[i])\n",
    "    plt.show()\n",
    "\n",
    "plot_video(vid_arr_col[0], (30,5), 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "frames_list = vid_arr_col[0]\n",
    "frames_array = np.array([frame.flatten() for frame in frames_list])\n",
    "# Instantiate t-SNE object with desired parameters\n",
    "print(2)\n",
    "tsne = TSNE(n_components=2, perplexity=10, random_state=0)\n",
    "\n",
    "# Fit t-SNE on the frames array\n",
    "frames_tsne = tsne.fit_transform(frames_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming labels is a list of labels corresponding to each frame\n",
    "plt.scatter(frames_tsne[:,0], frames_tsne[:,1], c=range(0,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(vid_arr):\n",
    "    flatten_first_video_frame = vid_arr[0][0]\n",
    "    flatten_first_video_frame = flatten_first_video_frame / 255\n",
    "    return np.array([flatten_first_video_frame])\n",
    "first_frame_formatted = format_frames(vid_arr_col)\n",
    "first_frame_formatted.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD MODEL\n",
    "- Add Layers\n",
    "- Add Loss function, optimizers, and metrics\n",
    "- Compile model and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_class = compute_class_weight(class_weight='balanced',classes=[0,1],y=np.argmax(y, axis=1))\n",
    "class_weights = dict(zip(np.unique(y), weight_class))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3D_model(input_data):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Conv3D(filters=64, kernel_size=3, padding=\"same\", strides=1, activation=\"relu\", input_shape=input_data.shape[1:]))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Conv3D(filters=8, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool3D(pool_size=2, padding=\"same\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(2 ,activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(input_data):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Conv3D(filters=32, kernel_size=(2,2,2),input_shape=input_data.shape[1:],\n",
    "                activation='relu',\n",
    "                padding='same', data_format='channels_last'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ConvLSTM2D(filters=16, kernel_size=(2, 2),\n",
    "                    padding='same', return_sequences=True))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "    model.add(layers.ConvLSTM2D(filters=16, kernel_size=(2, 2),\n",
    "                    padding='same', return_sequences=True))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2,2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dense(32,activation='elu'))\n",
    "\n",
    "    model.add(layers.Dense(2,activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_15 (Conv3D)          (None, 10, 30, 53, 32)    800       \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 10, 30, 53, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv_lstm2d_6 (ConvLSTM2D)  (None, 10, 30, 53, 16)    12352     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 10, 30, 53, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 10, 30, 53, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_18 (MaxPoolin  (None, 5, 15, 26, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, 5, 15, 26, 16)     8256      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 5, 15, 26, 16)    64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 5, 15, 26, 16)     0         \n",
      "                                                                 \n",
      " max_pooling3d_19 (MaxPoolin  (None, 2, 7, 13, 16)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 2912)              0         \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 2912)             11648     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                93216     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 126,594\n",
      "Trainable params: 120,642\n",
      "Non-trainable params: 5,952\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_m = test_model(vid_arr_col)\n",
    "test_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "optimizer = \"adam\"\n",
    "metrics=[\"accuracy\"]\n",
    "test_m.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 - 41s - loss: 0.3420 - accuracy: 0.8600 - 41s/epoch - 4s/step\n",
      "Epoch 2/10\n",
      "10/10 - 32s - loss: 0.3546 - accuracy: 0.7900 - 32s/epoch - 3s/step\n",
      "Epoch 3/10\n",
      "10/10 - 28s - loss: 0.4746 - accuracy: 0.8400 - 28s/epoch - 3s/step\n",
      "Epoch 4/10\n",
      "10/10 - 28s - loss: 0.3243 - accuracy: 0.8200 - 28s/epoch - 3s/step\n",
      "Epoch 5/10\n",
      "10/10 - 29s - loss: 0.2610 - accuracy: 0.8500 - 29s/epoch - 3s/step\n",
      "Epoch 6/10\n",
      "10/10 - 28s - loss: 0.3257 - accuracy: 0.8700 - 28s/epoch - 3s/step\n",
      "Epoch 7/10\n",
      "10/10 - 28s - loss: 0.2385 - accuracy: 0.9000 - 28s/epoch - 3s/step\n",
      "Epoch 8/10\n",
      "10/10 - 28s - loss: 0.2795 - accuracy: 0.8800 - 28s/epoch - 3s/step\n",
      "Epoch 9/10\n",
      "10/10 - 28s - loss: 0.3425 - accuracy: 0.8500 - 28s/epoch - 3s/step\n",
      "Epoch 10/10\n",
      "10/10 - 29s - loss: 0.2842 - accuracy: 0.8700 - 29s/epoch - 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf40182d90>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_m.fit(vid_arr_col, y[:vid_arr_col.shape[0]],epochs=10, batch_size=10, verbose=2, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 8s 2s/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = test_m.predict(vid_arr_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5452956 , 0.58010143],\n",
       "       [0.69432515, 0.5103222 ],\n",
       "       [0.6898619 , 0.5498576 ],\n",
       "       [0.5672844 , 0.5502688 ],\n",
       "       [0.4724289 , 0.67823744],\n",
       "       [0.6684107 , 0.51984555],\n",
       "       [0.5200045 , 0.56388307],\n",
       "       [0.47418377, 0.75669813],\n",
       "       [0.72182727, 0.32549965],\n",
       "       [0.6517231 , 0.47415045],\n",
       "       [0.5708572 , 0.54916346],\n",
       "       [0.8069315 , 0.40341088],\n",
       "       [0.44861883, 0.73705006],\n",
       "       [0.7271262 , 0.6431659 ],\n",
       "       [0.47014728, 0.6755512 ],\n",
       "       [0.6798664 , 0.67255026],\n",
       "       [0.45384347, 0.7337148 ],\n",
       "       [0.79489064, 0.5132833 ],\n",
       "       [0.470556  , 0.596708  ],\n",
       "       [0.6411012 , 0.5683278 ],\n",
       "       [0.72784674, 0.35633662],\n",
       "       [0.6141314 , 0.6496532 ],\n",
       "       [0.4730532 , 0.72429895],\n",
       "       [0.6476595 , 0.4716566 ],\n",
       "       [0.67235714, 0.6917969 ],\n",
       "       [0.67030287, 0.38449252],\n",
       "       [0.736177  , 0.42107755],\n",
       "       [0.52425   , 0.57936275],\n",
       "       [0.56352276, 0.553846  ],\n",
       "       [0.5896071 , 0.6118345 ],\n",
       "       [0.5667336 , 0.60006577],\n",
       "       [0.52912265, 0.5634136 ],\n",
       "       [0.65427995, 0.52177674],\n",
       "       [0.5595888 , 0.60574627],\n",
       "       [0.7282391 , 0.3418432 ],\n",
       "       [0.7217643 , 0.33459657],\n",
       "       [0.7468552 , 0.57660085],\n",
       "       [0.6539508 , 0.52262574],\n",
       "       [0.73758197, 0.53818727],\n",
       "       [0.5300641 , 0.56805843],\n",
       "       [0.47675094, 0.6939497 ],\n",
       "       [0.5704981 , 0.5496619 ],\n",
       "       [0.69646454, 0.69071686],\n",
       "       [0.7158162 , 0.5555208 ],\n",
       "       [0.6469179 , 0.68969357],\n",
       "       [0.54383785, 0.6308246 ],\n",
       "       [0.4848161 , 0.67031246],\n",
       "       [0.64391935, 0.6116671 ],\n",
       "       [0.46711385, 0.73241735],\n",
       "       [0.47360805, 0.7567228 ],\n",
       "       [0.6552906 , 0.5221942 ],\n",
       "       [0.44996893, 0.60544986],\n",
       "       [0.6210732 , 0.46312755],\n",
       "       [0.55615   , 0.5604743 ],\n",
       "       [0.63045096, 0.5837621 ],\n",
       "       [0.581505  , 0.51344115],\n",
       "       [0.5275925 , 0.6004862 ],\n",
       "       [0.7047705 , 0.42994314],\n",
       "       [0.5677614 , 0.55101645],\n",
       "       [0.5367511 , 0.6414255 ],\n",
       "       [0.6535735 , 0.54815346],\n",
       "       [0.59195435, 0.63996994],\n",
       "       [0.5594038 , 0.6066063 ],\n",
       "       [0.565604  , 0.6796967 ],\n",
       "       [0.6522908 , 0.54862833],\n",
       "       [0.4196898 , 0.6611626 ],\n",
       "       [0.69436294, 0.5112079 ],\n",
       "       [0.6826724 , 0.7155548 ],\n",
       "       [0.47372165, 0.6982181 ],\n",
       "       [0.6094838 , 0.6070259 ],\n",
       "       [0.5502375 , 0.55679214],\n",
       "       [0.5689989 , 0.5516918 ],\n",
       "       [0.47949576, 0.7453565 ],\n",
       "       [0.47888452, 0.7511264 ],\n",
       "       [0.6321869 , 0.5658737 ],\n",
       "       [0.6802335 , 0.7027215 ],\n",
       "       [0.65512764, 0.5224515 ],\n",
       "       [0.6159732 , 0.35547304],\n",
       "       [0.71537966, 0.351017  ],\n",
       "       [0.56967974, 0.55224043],\n",
       "       [0.73310566, 0.4385453 ],\n",
       "       [0.6853091 , 0.6781707 ],\n",
       "       [0.6139143 , 0.4757898 ],\n",
       "       [0.58083665, 0.5142175 ],\n",
       "       [0.64150006, 0.57050157],\n",
       "       [0.45535222, 0.7319059 ],\n",
       "       [0.6325335 , 0.58199304],\n",
       "       [0.6710228 , 0.5641819 ],\n",
       "       [0.65260196, 0.44375846],\n",
       "       [0.5420742 , 0.69939303],\n",
       "       [0.5713792 , 0.54997826],\n",
       "       [0.61450344, 0.47410756],\n",
       "       [0.64776754, 0.7513252 ],\n",
       "       [0.5007223 , 0.59414184],\n",
       "       [0.7008373 , 0.5535133 ],\n",
       "       [0.6680679 , 0.43612006],\n",
       "       [0.6108481 , 0.41230637],\n",
       "       [0.70373607, 0.4377135 ],\n",
       "       [0.73269784, 0.32566312],\n",
       "       [0.5810634 , 0.51950794]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.288     0.895     0.436        19\n",
      "           1      0.951     0.481     0.639        81\n",
      "\n",
      "    accuracy                          0.560       100\n",
      "   macro avg      0.620     0.688     0.538       100\n",
      "weighted avg      0.825     0.560     0.601       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual_y = np.argmax(y[:vid_arr_col.shape[0]], axis=1)\n",
    "pred_y = np.argmax(pred_y, axis=1)\n",
    "print(classification_report(actual_y, pred_y, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(actual_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fdf40182640>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAGwCAYAAAAqpFaiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5fklEQVR4nO3deXRUVbr38V9lqoQkFQxIBikiECHMKK0YlSGKEuylIBGvDAo04FUBBZrRFgRsjVOLwsXoxUDAFhERaMGBF8FEELAFjYIMGkQJEsALTUICGUid9w+ktAxDFXVCKsX3s9ZeK3WGfZ6jIXny7H32sRiGYQgAAOA8Amo6AAAAUDuQNAAAALeQNAAAALeQNAAAALeQNAAAALeQNAAAALeQNAAAALcE1XQAtYXD4dD+/fsVGRkpi8VS0+EAADxkGIaOHTum+Ph4BQRUz9/MpaWlKi8vN6WvkJAQhYaGmtKXWUga3LR//37Z7faaDgMA4KX8/Hw1bNjQ9H5LS0vVOCFCBw5VmtJfbGys9uzZ41OJA0mDmyIjIyVJzYZNUWCI7/wPBMwUM/vzmg4BqDYnVaH1+sD589xs5eXlOnCoUj9tuVK2SO8qGUXHHEro8KPKy8tJGmqj00MSgSGhCrT6zv9AwExBluCaDgGoPr++NKG6h5gjIi2KiPTuGg755jA4SQMAACaqNByq9PKtTpWGw5xgTEbSAACAiRwy5JB3WYO351cXHrkEAABuodIAAICJHHLI28EF73uoHiQNAACYqNIwVGl4N7zg7fnVheEJAADgFioNAACYyJ8nQpI0AABgIocMVfpp0sDwBAAAcAuVBgAATMTwBAAAcAtPTwAAgEseSQMAACZymNQu1DPPPCOLxaJRo0Y5t5WWlmr48OGqV6+eIiIilJaWpoMHD3rcN0kDAAAmqvz16Qlv24X44osv9Nprr6lt27Yu20ePHq0VK1bonXfeUU5Ojvbv36/evXt73D9JAwAAJqo0zGmeKi4uVv/+/TVnzhxddtllzu2FhYXKzMzUiy++qJtvvlkdOnTQvHnztGHDBm3atMmja5A0AADgo4qKilxaWVnZWY8dPny4/vznP6tbt24u27ds2aKKigqX7UlJSWrUqJE2btzoUTwkDQAAmMjMOQ12u11RUVHOlp6efsZrLlq0SF9++eUZ9x84cEAhISGqW7euy/aYmBgdOHDAo3vjkUsAAEzkkEWVsnjdhyTl5+fLZrM5t1ut1irH5ufn69FHH9Xq1asVGhrq1XXPh0oDAAA+ymazubQzJQ1btmzRoUOHdM011ygoKEhBQUHKycnRzJkzFRQUpJiYGJWXl+vo0aMu5x08eFCxsbEexUOlAQAAEzmMU83bPtx1yy23aOvWrS7bBg8erKSkJE2YMEF2u13BwcFas2aN0tLSJEm7du3S3r17lZyc7FFcJA0AAJio0oThCU/Oj4yMVOvWrV22hYeHq169es7tQ4YM0ZgxYxQdHS2bzaaRI0cqOTlZ119/vUdxkTQAAODnZsyYoYCAAKWlpamsrEzdu3fXK6+84nE/JA0AAJjoYlcaziQ7O9vlc2hoqGbPnq3Zs2d71S9JAwAAJnIYFjkML5+e8PL86sLTEwAAwC1UGgAAMJEvDE9UF5IGAABMVKkAVXpZyK80KRazkTQAAGAiw4Q5DQZzGgAAQG1GpQEAABMxpwEAALil0ghQpeHlnAYvl6GuLgxPAAAAt1BpAADARA5Z5PDyb3KHfLPUQNIAAICJ/HlOA8MTAADALVQaAAAwkTkTIRmeAADA752a0+DlC6sYngAAALUZlQYAAEzkMOHdEzw9AQDAJYA5DQAAwC0OBfjtOg3MaQAAAG6h0gAAgIkqDYsqvXy1tbfnVxeSBgAATFRpwkTISoYnAABAbUalAQAAEzmMADm8fHrCwdMTAAD4P4YnAADAJY9KAwAAJnLI+6cfHOaEYjqSBgAATGTO4k6+ORDgm1EBAACfQ6UBAAATmfPuCd/8m56kAQAAEzlkkUPezmlgRUgAAPyeP1cafDMqAADgc6g0AABgInMWd/LNv+lJGgAAMJHDsMjh7ToNPvqWS99MZQAAgM8haQAAwESOX4cnvGmeLu6UkZGhtm3bymazyWazKTk5WR9++KFzf9euXWWxWFzagw8+6PG9MTwBAICJzHnLpWfnN2zYUM8884yuuuoqGYah+fPnq2fPnvrqq6/UqlUrSdKwYcM0ffp05zl16tTxOC6SBgAAark77rjD5fNTTz2ljIwMbdq0yZk01KlTR7GxsV5dh+EJAABMVCmLKU2SioqKXFpZWdn5r19ZqUWLFqmkpETJycnO7W+++abq16+v1q1ba9KkSTp+/LjH90alAQAAE5k5PGG32122P/HEE5o6deoZz9m6dauSk5NVWlqqiIgILVu2TC1btpQk9evXTwkJCYqPj9c333yjCRMmaNeuXVq6dKlHcZE0AADgo/Lz82Wz2ZyfrVbrWY9t3ry5cnNzVVhYqCVLlmjgwIHKyclRy5Yt9cADDziPa9OmjeLi4nTLLbdo9+7datq0qdvxkDQAAGCiSsk5vOBNH5KcT0O4IyQkRImJiZKkDh066IsvvtDLL7+s1157rcqxHTt2lCTl5eWRNAAAUFNq4umJM/bhcJx1DkRubq4kKS4uzqM+SRoAADBRTbywatKkSerRo4caNWqkY8eOaeHChcrOztaqVau0e/duLVy4ULfffrvq1aunb775RqNHj1bnzp3Vtm1bj65D0gAAQC136NAh3X///SooKFBUVJTatm2rVatW6dZbb1V+fr4+/vhjvfTSSyopKZHdbldaWpoef/xxj69D0gAAgIkMWeTwck6D4eH5mZmZZ91nt9uVk5PjVTynkTQAAGCimhieuFh8MyoAAOBzqDQAAGAif341NkkDAAAmOv2mSm/78EW+GRUAAPA5VBoAADARwxMAAMAtDgXI4WUh39vzq4tvRgUAAHwOlQYAAExUaVhU6eXwgrfnVxeSBgAATMScBgAA4BbDhLdcGqwICQAAajMqDQAAmKhSFlV6+cIqb8+vLiQNAACYyGF4PyfBYZgUjMkYngAAAG6h0oAadU3D/Rp0ba5axP6iBhHHNWpZqj7Ja+zc//W4jDOe92L29Zr/xdUXK0zANP814qBuvL1Q9sQylZcGaPvmOsp8Kk77dofWdGgwicOEiZDenl9dSBpQo8KCK7Trl3pavi1JM3qtqrL/5lcGuny+qfFeTU39RB9/1/RihQiYqm1yiVZk1dd3uXUUGGRo0MQCPf3WDxrWpbnKTgTWdHgwgUMWObyck+Dt+dWlRlOZQYMGyWKxyGKxKDg4WI0bN9b48eNVWlrqPOb0/j+2RYsWVekvKSlJVqtVBw4cqLKva9euGjVqVHXeDi7AZ3sSNHt9R639vskZ9x8uqePSuibu0Rd7r9DPhbaLHClgjr/1b6LVi6P103eh+mF7mP4xqpFiGlboqrYnajo04LxqvP6RmpqqgoIC/fDDD5oxY4Zee+01PfHEEy7HzJs3TwUFBS6tV69eLsesX79eJ06c0N1336358+dfxDvAxRJd57g6NdmrZVuTajoUwDThtkpJ0rGjVBn8xekVIb1tvqjGkwar1arY2FjZ7Xb16tVL3bp10+rVq12OqVu3rmJjY11aaKjr+F9mZqb69eun++67T3Pnzr2Yt4CL5M7Wu3S8PFhrvjtzVQKobSwWQw9O+1nb/l1HP+0Kq+lwYJLTcxq8bb7Ip+Y0bNu2TRs2bFBCQoJH5x07dkzvvPOOPv/8cyUlJamwsFDr1q1Tp06dLjiWsrIylZWVOT8XFRVdcF8wR6/WO/XBjqtUXulT37bABRvx9M9KSCrVX3sl1nQogFtqPJVZuXKlIiIiFBoaqjZt2ujQoUMaN26cyzF9+/ZVRESES9u7d69z/6JFi3TVVVepVatWCgwM1L333qvMzEyv4kpPT1dUVJSz2e12r/qDd66+Yr8a1zuqpd+0qOlQAFMMf2qfOt5apPF3N9X/FYTUdDgwkUMW5/snLrj56ETIGv+TLSUlRRkZGSopKdGMGTMUFBSktLQ0l2NmzJihbt26uWyLj493fj137lwNGDDA+XnAgAHq0qWLZs2apcjIyAuKa9KkSRozZozzc1FREYlDDbqr7U59e+ByffdL/ZoOBfCSoeFP/awbUgs17u5EHcy31nRAMJlhwtMTBknDmYWHhysx8VRpbu7cuWrXrp0yMzM1ZMgQ5zGxsbHOY/5o+/bt2rRpk/79739rwoQJzu2VlZVatGiRhg0bdkFxWa1WWa38Y65uYcEVanRZofPzFVFFat7g/1R4wqoDx04lfOEh5bqt2W79I/uGmgoTMM2Ip39Wyl3/0dTBjXWiOECXXV4hSSo5Fqjy0hov/sIEvOXyIgkICNBjjz2mMWPGqF+/fgoLO//EoMzMTHXu3FmzZ8922T5v3jxlZmZecNKAi6NV7CFl3vue8/O4mzdIkv61rbmmfHizJCk1KU+ySB/uYNwXtd8dgw5Lkl5Yuttl+wuj7Fq9OLomQgLc5lNJgyT16dNH48aN0+zZszV27FhJ0tGjR6usvRAZGamQkBC98cYbmj59ulq3bu2yf+jQoXrxxRf17bffqlWrVpKkX375Rbm5uS7HxcXFKSYmpvpuCOe0Of8KtXv+oXMe8+43LfXuNy0vUkRA9eoe366mQ0A18+cVIX0uqqCgII0YMULPPfecSkpKJEmDBw9WXFycS5s1a5bee+89HT58WHfddVeVflq0aKEWLVq4TIhcuHChrr76apc2Z86ci3ZvAAD/5/UkSBOGN6pLjVYasrKyzrh94sSJmjhxoiTJMM79qq/Kysqz7tu+fbvz6+zsbI/jAwAAv/G54QkAAGozf373BEkDAAAm8uenJ3xuTgMAAPBNVBoAADCRP1caSBoAADCRPycNDE8AAAC3UGkAAMBEVBoAAIBbDP322OWFtnOvUFRVRkaG2rZtK5vNJpvNpuTkZH344YfO/aWlpRo+fLjq1auniIgIpaWl6eDBgx7fG0kDAAAmqokVIRs2bKhnnnlGW7Zs0ebNm3XzzTerZ8+e+vbbbyVJo0eP1ooVK/TOO+8oJydH+/fvV+/evT2+N4YnAACo5e644w6Xz0899ZQyMjK0adMmNWzYUJmZmVq4cKFuvvnUiwDnzZunFi1aaNOmTbr++uvdvg5JAwAAJjJzTkNRUZHLdqvVKqvVes5zKysr9c4776ikpETJycnasmWLKioq1K1bN+cxSUlJatSokTZu3OhR0sDwBAAAJjJzeMJutysqKsrZ0tPTz3rdrVu3KiIiQlarVQ8++KCWLVumli1b6sCBAwoJCVHdunVdjo+JianyBunzodIAAICPys/Pl81mc34+V5WhefPmys3NVWFhoZYsWaKBAwcqJyfH1HhIGgAAMJGZwxOnn4ZwR0hIiBITEyVJHTp00BdffKGXX35Z//Vf/6Xy8nIdPXrUpdpw8OBBxcbGehQXwxMAAJjIMCymNG85HA6VlZWpQ4cOCg4O1po1a5z7du3apb179yo5OdmjPqk0AABQy02aNEk9evRQo0aNdOzYMS1cuFDZ2dlatWqVoqKiNGTIEI0ZM0bR0dGy2WwaOXKkkpOTPZoEKZE0AABgqtMLNHnbhycOHTqk+++/XwUFBYqKilLbtm21atUq3XrrrZKkGTNmKCAgQGlpaSorK1P37t31yiuveBwXSQMAACaqiWWkMzMzz7k/NDRUs2fP1uzZs70JizkNAADAPVQaAAAwkRkTGc2YCFkdSBoAADCRP7/lkqQBAAAT+XOlgTkNAADALVQaAAAwkWHC8ISvVhpIGgAAMJEhyTC878MXMTwBAADcQqUBAAATOWSR5SKvCHmxkDQAAGAinp4AAACXPCoNAACYyGFYZGFxJwAAcD6GYcLTEz76+ATDEwAAwC1UGgAAMJE/T4QkaQAAwEQkDQAAwC3+PBGSOQ0AAMAtVBoAADCRPz89QdIAAICJTiUN3s5pMCkYkzE8AQAA3EKlAQAAE/H0BAAAcIvxa/O2D1/E8AQAAHALlQYAAEzE8AQAAHCPH49PkDQAAGAmEyoN8tFKA3MaAACAW6g0AABgIlaEBAAAbvHniZAMTwAAALdQaQAAwEyGxfuJjD5aaSBpAADARP48p4HhCQAA4BYqDQAAmInFnQAAgDv8+ekJt5KG9957z+0O77zzzgsOBgAAeC49PV1Lly7Vzp07FRYWphtuuEHPPvusmjdv7jyma9euysnJcTnvv//7v/Xqq6+6fR23koZevXq51ZnFYlFlZaXbFwcAwC9d5OGFnJwcDR8+XNdee61Onjypxx57TLfddpu2b9+u8PBw53HDhg3T9OnTnZ/r1Knj0XXcShocDodHnQIAcKkyc3iiqKjIZbvVapXVaq1y/EcffeTyOSsrSw0aNNCWLVvUuXNn5/Y6deooNjb2guPy6umJ0tJSb04HAMD/GCY1SXa7XVFRUc6Wnp7uVgiFhYWSpOjoaJftb775purXr6/WrVtr0qRJOn78uEe35vFEyMrKSj399NN69dVXdfDgQX333Xdq0qSJJk+erCuvvFJDhgzxtEsAAHAG+fn5stlszs9nqjL8kcPh0KhRo3TjjTeqdevWzu39+vVTQkKC4uPj9c0332jChAnatWuXli5d6nY8HicNTz31lObPn6/nnntOw4YNc25v3bq1XnrpJZIGAMAlzvJr87YPyWazuSQN7hg+fLi2bdum9evXu2x/4IEHnF+3adNGcXFxuuWWW7R79241bdrUrb49Hp5YsGCB/vd//1f9+/dXYGCgc3u7du20c+dOT7sDAMC/mDg84akRI0Zo5cqV+uSTT9SwYcNzHtuxY0dJUl5entv9e1xp+Pnnn5WYmFhlu8PhUEVFhafdAQAALxmGoZEjR2rZsmXKzs5W48aNz3tObm6uJCkuLs7t63icNLRs2VLr1q1TQkKCy/YlS5bo6quv9rQ7AAD8Sw2sCDl8+HAtXLhQ//rXvxQZGakDBw5IkqKiohQWFqbdu3dr4cKFuv3221WvXj198803Gj16tDp37qy2bdu6fR2Pk4YpU6Zo4MCB+vnnn+VwOLR06VLt2rVLCxYs0MqVKz3tDgAA/1IDb7nMyMiQdGoBp9+bN2+eBg0apJCQEH388cd66aWXVFJSIrvdrrS0ND3++OMeXcfjpKFnz55asWKFpk+frvDwcE2ZMkXXXHONVqxYoVtvvdXT7gAAgJeM87wW0263V1kN8kJc0LsnOnXqpNWrV3t9cQAA/I0/vxr7gl9YtXnzZu3YsUPSqXkOHTp0MC0oAABqLd5y+Zt9+/apb9+++uyzz1S3bl1J0tGjR3XDDTdo0aJF533EAwAA1E4er9MwdOhQVVRUaMeOHTpy5IiOHDmiHTt2yOFwaOjQodURIwAAtcfpiZDeNh/kcaUhJydHGzZscHndZvPmzTVr1ix16tTJ1OAAAKhtLMap5m0fvsjjpMFut59xEafKykrFx8ebEhQAALWWH89p8Hh44vnnn9fIkSO1efNm57bNmzfr0Ucf1QsvvGBqcAAAwHe4VWm47LLLZLH8Nr5SUlKijh07Kijo1OknT55UUFCQ/vKXv6hXr17VEigAALVCDSzudLG4lTS89NJL1RwGAAB+wo+HJ9xKGgYOHFjdcQAAAB93wYs7SVJpaanKy8tdtnn63m8AAPyKH1caPJ4IWVJSohEjRqhBgwYKDw/XZZdd5tIAALikGSY1H+Rx0jB+/HitXbtWGRkZslqtev311zVt2jTFx8drwYIF1REjAADwAR4PT6xYsUILFixQ165dNXjwYHXq1EmJiYlKSEjQm2++qf79+1dHnAAA1A5+/PSEx5WGI0eOqEmTJpJOzV84cuSIJOmmm27Sp59+am50AADUMqdXhPS2+SKPk4YmTZpoz549kqSkpCQtXrxY0qkKxOkXWAEAAP/jcdIwePBgff3115KkiRMnavbs2QoNDdXo0aM1btw40wMEAKBW8eOJkB7PaRg9erTz627dumnnzp3asmWLEhMT1bZtW1ODAwAAvsOrdRokKSEhQQkJCWbEAgBArWeRCW+5NCUS87mVNMycOdPtDh955JELDgYAAPgut5KGGTNmuNWZxWLx+6Rhw6Ovyxbp8VQQoFbIfaSspkMAqk3xMYe6tLkIF/LjRy7dShpOPy0BAADOg2WkAQDApc7riZAAAOB3/LjSQNIAAICJzFjR0W9WhAQAAJcmKg0AAJjJj4cnLqjSsG7dOg0YMEDJycn6+eefJUlvvPGG1q9fb2pwAADUOn68jLTHScO7776r7t27KywsTF999ZXKyk49111YWKinn37a9AABAIBv8Dhp+Pvf/65XX31Vc+bMUXBwsHP7jTfeqC+//NLU4AAAqG38+dXYHs9p2LVrlzp37lxle1RUlI4ePWpGTAAA1F5+vCKkx5WG2NhY5eXlVdm+fv16NWnSxJSgAACotZjT8Jthw4bp0Ucf1eeffy6LxaL9+/frzTff1NixY/XQQw9VR4wAAMAHeDw8MXHiRDkcDt1yyy06fvy4OnfuLKvVqrFjx2rkyJHVESMAALWGPy/u5HHSYLFY9Le//U3jxo1TXl6eiouL1bJlS0VERFRHfAAA1C6s01BVSEiIWrZsqeuuu46EAQCAGpSenq5rr71WkZGRatCggXr16qVdu3a5HFNaWqrhw4erXr16ioiIUFpamg4ePOjRdTyuNKSkpMhiOfuszrVr13raJQAA/sOMRyY9PD8nJ0fDhw/Xtddeq5MnT+qxxx7Tbbfdpu3btys8PFySNHr0aL3//vt65513FBUVpREjRqh379767LPP3L6Ox0lD+/btXT5XVFQoNzdX27Zt08CBAz3tDgAA/2Li8ERRUZHLZqvVKqvVWuXwjz76yOVzVlaWGjRooC1btqhz584qLCxUZmamFi5cqJtvvlmSNG/ePLVo0UKbNm3S9ddf71ZYHicNM2bMOOP2qVOnqri42NPuAADAWdjtdpfPTzzxhKZOnXre8woLCyVJ0dHRkqQtW7aooqJC3bp1cx6TlJSkRo0aaePGjdWXNJzNgAEDdN111+mFF14wq0sAAGofEysN+fn5stlszs1nqjL8kcPh0KhRo3TjjTeqdevWkqQDBw4oJCREdevWdTk2JiZGBw4ccDss05KGjRs3KjQ01KzuAAColcx85NJms7kkDe4YPny4tm3bVi0vkfQ4aejdu7fLZ8MwVFBQoM2bN2vy5MmmBQYAADwzYsQIrVy5Up9++qkaNmzo3B4bG6vy8nIdPXrUpdpw8OBBxcbGut2/x0lDVFSUy+eAgAA1b95c06dP12233eZpdwAAwEuGYWjkyJFatmyZsrOz1bhxY5f9HTp0UHBwsNasWaO0tDRJp94ltXfvXiUnJ7t9HY+ShsrKSg0ePFht2rTRZZdd5smpAABcGmpgcafhw4dr4cKF+te//qXIyEjnPIWoqCiFhYUpKipKQ4YM0ZgxYxQdHS2bzaaRI0cqOTnZ7UmQkodJQ2BgoG677Tbt2LGDpAEAgDOoiWWkMzIyJEldu3Z12T5v3jwNGjRI0qmnHwMCApSWlqaysjJ1795dr7zyikfX8Xh4onXr1vrhhx+qlD4AAEDNMIzzZxmhoaGaPXu2Zs+efcHX8XgZ6b///e8aO3asVq5cqYKCAhUVFbk0AAAueX74WmzJg0rD9OnT9de//lW33367JOnOO+90WU7aMAxZLBZVVlaaHyUAALWFH7+wyu2kYdq0aXrwwQf1ySefVGc8AADAR7mdNJweL+nSpUu1BQMAQG1XExMhLxaPJkKe6+2WAABADE+c1qxZs/MmDkeOHPEqIAAA4Js8ShqmTZtWZUVIAADwG4YnfnXvvfeqQYMG1RULAAC1nx8PT7i9TgPzGQAAuLR5/PQEAAA4Bz+uNLidNDgcjuqMAwAAv8CcBgAA4B4/rjR4/O4JAABwaaLSAACAmfy40kDSAACAifx5TgPDEwAAwC1UGgAAMBPDEwAAwB0MTwAAgEselQYAAMzE8AQAAHCLHycNDE8AAAC3UGkAAMBEll+bt334IpIGAADM5MfDEyQNAACYiEcuAQDAJY9KAwAAZmJ4AgAAuM1Hf+l7i+EJAADgFioNAACYyJ8nQpI0AABgJj+e08DwBAAAcAuVBgAATMTwBAAAcA/DEwAA4FJH0gAAgIlOD0942zzx6aef6o477lB8fLwsFouWL1/usn/QoEGyWCwuLTU11eN7I2kAAMBMhknNAyUlJWrXrp1mz5591mNSU1NVUFDgbG+99ZZnFxFzGgAAMFcNzGno0aOHevTocc5jrFarYmNjvQiKSgMAAD6rqKjIpZWVlV1wX9nZ2WrQoIGaN2+uhx56SIcPH/a4D5IGAABMZOacBrvdrqioKGdLT0+/oJhSU1O1YMECrVmzRs8++6xycnLUo0cPVVZWetQPwxMAAJjJxOGJ/Px82Ww252ar1XpB3d17773Or9u0aaO2bduqadOmys7O1i233OJ2P1QaAADwUTabzaVdaNLwR02aNFH9+vWVl5fn0XlUGgAAMJHFMGQxvCs1eHv++ezbt0+HDx9WXFycR+eRNAAAYKYaeHqiuLjYpWqwZ88e5ebmKjo6WtHR0Zo2bZrS0tIUGxur3bt3a/z48UpMTFT37t09ug5JAwAAtdzmzZuVkpLi/DxmzBhJ0sCBA5WRkaFvvvlG8+fP19GjRxUfH6/bbrtNTz75pMfDHSQNAACYqCZeWNW1a1cZ5xjSWLVqlXcB/YqkAQAAM/HCKgAAcKmj0gAAgIlqYnjiYiFpAADATH48PEHSAACAify50sCcBgAA4BYqDQAAmInhCQAA4C5fHV7wFsMTAADALVQaAAAwk2Gcat724YNIGgAAMBFPTwAAgEselQYAAMzE0xMAAMAdFsep5m0fvojhCQAA4BYqDfAZb89qoLnp8eo19Bc9NP1nFf0nUG+8EKsvcyJ1aH+IoqJP6obUQg0cX6Bwm4+m4cDvbPxnA236Z4z+87NVkhRz1XHd8sjPSupaKEk6/JNV7z/dSD9ujtTJ8gA163xUPaf+qMjLT9Zk2PAWwxNA9dqVG6b3/1lPjVuecG47cjBYhw8Ga9iU/WrUrFSH9oVo5sSGOnwwWJPn/FhzwQJuiootV48Je1X/ylIZhkVb3q2vBQ800yMrtym6YZlevz9JcS2Oa9ibOyRJ/+/Fhsoa2lzDl32rAOrAtRZPT1SzQYMGyWKxVGl5eXmSpPT0dAUGBur555+vcm5WVpbq1q3rsm3Hjh2y2+3q06ePysvLlZWVdcb+Q0NDL8bt4TxOlATo2REJGvV8viKjKp3br0wq1ZTXf9T1txUp/spytb+pWIMmFOjz1TZV8ocYaoGW3Y4qKaVQ9RuX6fImpUodt08hdRza+1WEftwcqf/ss+qe539QXNIJxSWd0D0v/KCft4Zr9wZbTYcOb5xep8Hb5oN8ImmQpNTUVBUUFLi0xo0bS5Lmzp2r8ePHa+7cueft54svvlCnTp2Umpqqt99+WyEhIZIkm81Wpf+ffvqpWu8J7vmfxxrquluKdE3n4vMeW1IUqDoRDgVSI0Mt46iUcldEq/xEgBKuKdbJcossFiko5LehtmCrQ5YA6cfNkTUYKXB2PvOj12q1KjY2tsr2nJwcnThxQtOnT9eCBQu0YcMG3XDDDWfsY+3aterZs6cefvhhPfvssy77LBbLGfs/m7KyMpWVlTk/FxUVuX0u3Je9vK7ytoZp1gffnffYwsOBWvhSrHoM+L+LEBlgjoKdYXolrZVOlgUopE6l7n/1O8VcdULh0RUKrlOpD561K3XcPsmQPnzWLkelRUWHgms6bHiB4YkalJmZqb59+yo4OFh9+/ZVZmbmGY9btmyZ/vznP+vxxx+vkjBciPT0dEVFRTmb3W73uk+4OvRzsDKmXKEJ//OTQkLP/S+k5FiAJt/fRI2aleq+vx64SBEC3ru8SakefX+rhi/bpusHHNLisU118PswRdQ7qQH/k6cday7TlFZ/0hNt/6QTRUG6onUJ8xlqO8Ok5oN85ltz5cqVioiIcLY+ffqoqKhIS5Ys0YABAyRJAwYM0OLFi1Vc7FrGLi4uVp8+fTRu3DhNmDDhjP0XFha69B8REaEePXqcNZ5JkyapsLDQ2fLz8827WUiS8r6po6P/F6zh3Zurh72detjb6ZuNEfpXZn31sLdT5a/TG44XB+hv/ZoqLNyhJzL3KIg/wlCLBIUYqn9lmRq2Oa4e4/MV1+K41s+LkSQ161yoCTlfa/LmLzXlyy26d8ZuFR4IVrS97Dy9AjXDZ4YnUlJSlJGR4fwcHh6ut956S02bNlW7du0kSe3bt1dCQoLefvttDRkyxHlsWFiYbrrpJs2ZM0d9+/ZVixYtqvQfGRmpL7/80mVbWFjYWeOxWq2yWq3e3hbOoX2nY3pt7U6Xbf8Y3Uj2xFLdM/yQAgNPVRj+1q+pgkMMTcv64bwVCcDXGQ6pstz177Xw6FMze/M22FRyOFgtu/2nJkKDSfx5eMJnkobw8HAlJia6bMvMzNS3336roKDfwnQ4HJo7d65L0hAYGKjly5erd+/eSklJ0SeffFIlcQgICKjSP2pWnQiHrkwqddkWWsehyMsqdWVSqUqOBeixvk1VdiJA42ft0fHiQB3/tcgUVe+kAgNrIGjAAx8+Z1fzLkdV94oylRUHKve9+vphk01/mX8qWf7infpqkFiqiOgK/fRlhFZMT9BNfzmgy5uWnqdn+DTecnnxbd26VZs3b1Z2draio6Od248cOaKuXbtq586dSkpKcm63Wq1aunSp7r77bqWkpGjt2rVq2bJlTYQOk+RtraOdX4ZLkgbf4Pr/cv7n2xVrL6+JsAC3FR8O0uK/NlXRL8EKjaxUXNJx/WX+TjXrdGpi9f/9EKaPnrPrRGGQLruiTCnD96vTEObswHf5bNKQmZmp6667Tp07d66y79prr1VmZmaVdRusVqveffdd9enTx5k4tGrVSpJkGIYOHKj6j7FBgwYKYNaRz3j+3Tzn1+1uKNaq/bk1FwzgpT7P7jnn/h4T8tVjAvOl/I0/D0/45G/L8vJy/fOf/1RaWtoZ96elpWnBggWqqKiosi8kJERLlizRDTfcoJSUFG3btk3SqUcm4+LiqrRDhw5V670AAC4xfvz0hMUwfHTgxMcUFRUpKipK//muiWyRPplrAV7LLWPWPvxX8TGHurT5WYWFhbLZzF918/TvieTU6QoK9m7F4ZMVpdr40ZRqi/VC+ezwBAAAtZE/D0+QNAAAYCaHcap524cPImkAAMBMfvxqbAbnAQCAW6g0AABgIotMmNNgSiTmI2kAAMBMfrwiJMMTAADALSQNAACY6PQjl942T3z66ae64447FB8fL4vFouXLl7vsNwxDU6ZMUVxcnMLCwtStWzd9//33Ht8bSQMAAGaqgRUhS0pK1K5dO82ePfuM+5977jnNnDlTr776qj7//HOFh4ere/fuKi317OVozGkAAKCW69Gjh3r06HHGfYZh6KWXXtLjjz+unj17SpIWLFigmJgYLV++XPfee6/b16HSAACAiSyGYUqTTi1N/ftWdgFLve/Zs0cHDhxQt27dnNuioqLUsWNHbdy40aO+SBoAADCTw6QmyW63KyoqytnS09M9Duf0G55jYmJctsfExJzx7c/nwvAEAAA+Kj8/3+WFVVartQajIWkAAMBUvx9e8KYPSbLZbF6/5TI2NlaSdPDgQcXFxTm3Hzx4UO3bt/eoL4YnAAAwUw08PXEujRs3VmxsrNasWePcVlRUpM8//1zJycke9UWlAQAAM9XAipDFxcXKy8tzft6zZ49yc3MVHR2tRo0aadSoUfr73/+uq666So0bN9bkyZMVHx+vXr16eXQdkgYAAGq5zZs3KyUlxfl5zJgxkqSBAwcqKytL48ePV0lJiR544AEdPXpUN910kz766COFhoZ6dB2SBgAATHQhKzqeqQ9PdO3aVcY5qhMWi0XTp0/X9OnTvYqLpAEAADPxwioAAHCpo9IAAICJLI5Tzds+fBFJAwAAZmJ4AgAAXOqoNAAAYCYzFmfyzUIDSQMAAGYycxlpX8PwBAAAcAuVBgAAzOTHEyFJGgAAMJMhydtHJn0zZyBpAADATMxpAAAAlzwqDQAAmMmQCXMaTInEdCQNAACYyY8nQjI8AQAA3EKlAQAAMzkkWUzowweRNAAAYCKengAAAJc8Kg0AAJjJjydCkjQAAGAmP04aGJ4AAABuodIAAICZ/LjSQNIAAICZeOQSAAC4g0cuAQDAJY9KAwAAZmJOAwAAcIvDkCxe/tJ3+GbSwPAEAABwC5UGAADMxPAEAABwjwlJg3wzaWB4AgAAuIVKAwAAZmJ4AgAAuMVhyOvhBZ6eAAAAtRmVBgAAzGQ4TjVv+/BBVBoAADDT6TkN3jYPTJ06VRaLxaUlJSWZfmtUGgAAMFMNzWlo1aqVPv74Y+fnoCDzf8WTNAAA4AeCgoIUGxtbrddgeAIAADOZODxRVFTk0srKys562e+//17x8fFq0qSJ+vfvr71795p+ayQNAACYyZAJScOprux2u6KiopwtPT39jJfs2LGjsrKy9NFHHykjI0N79uxRp06ddOzYMVNvjeEJAAB8VH5+vmw2m/Oz1Wo943E9evRwft22bVt17NhRCQkJWrx4sYYMGWJaPCQNAACYycQVIW02m0vS4K66deuqWbNmysvL8y6OP2B4AgAAMzkc5jQvFBcXa/fu3YqLizPppk4haQAAoJYbO3ascnJy9OOPP2rDhg266667FBgYqL59+5p6HYYnAAAwUw28sGrfvn3q27evDh8+rMsvv1w33XSTNm3apMsvv9y7OP6ApAEAADPVQNKwaNEi767nJoYnAACAW6g0AABgJj9+NTZJAwAAJjIMhwwv31Lp7fnVhaQBAAAzGYb3lQJv50RUE+Y0AAAAt1BpAADATIYJcxp8tNJA0gAAgJkcDsni5ZwEH53TwPAEAABwC5UGAADMxPAEAABwh+FwyPByeMJXH7lkeAIAALiFSgMAAGZieAIAALjFYUgW/0waGJ4AAABuodIAAICZDEOSt+s0+GalgaQBAAATGQ5DhpfDEwZJAwAAlwDDIe8rDTxyCQAAajEqDQAAmIjhCQAA4B4/Hp4gaXDT6ayvqNg3/0cCZigu4/sb/qvk15/f1f1X/ElVeL2200lVmBOMyUga3HTs2DFJUsI1P9ZsIAAArxw7dkxRUVGm9xsSEqLY2FitP/CBKf3FxsYqJCTElL7MYjF8deDExzgcDu3fv1+RkZGyWCw1HY7fKyoqkt1uV35+vmw2W02HA5iO7/GLzzAMHTt2TPHx8QoIqJ7nAEpLS1VeXm5KXyEhIQoNDTWlL7NQaXBTQECAGjZsWNNhXHJsNhs/UOHX+B6/uKqjwvB7oaGhPveL3kw8cgkAANxC0gAAANxC0gCfZLVa9cQTT8hqtdZ0KEC14HsctRETIQEAgFuoNAAAALeQNAAAALeQNAAAALeQNAAAALeQNKDaDBo0SBaLRRaLRcHBwWrcuLHGjx+v0tJS5zGn9/+xLVq0qEp/SUlJslqtOnDgQJV9Xbt21ahRo6rzdoAqfv89/vuWl5cnSUpPT1dgYKCef/75KudmZWWpbt26Ltt27Nghu92uPn36qLy8XFlZWWfs358XD4JvI2lAtUpNTVVBQYF++OEHzZgxQ6+99pqeeOIJl2PmzZungoICl9arVy+XY9avX68TJ07o7rvv1vz58y/iHQDndvp7/PetcePGkqS5c+dq/Pjxmjt37nn7+eKLL9SpUyelpqbq7bffdr5zwGazVen/p59+qtZ7As6GpAHVymq1KjY2Vna7Xb169VK3bt20evVql2Pq1q2r2NhYl/bHv6QyMzPVr18/3XfffW79AAYultPf479vgYGBysnJ0YkTJzR9+nQVFRVpw4YNZ+1j7dq1uvnmmzVkyBDNmTPH5b0IFoulSv8xMTEX49aAKkgacNFs27ZNGzZs8PitbceOHdM777yjAQMG6NZbb1VhYaHWrVtXTVEC5sjMzFTfvn0VHBysvn37KjMz84zHLVu2TH/+85/1+OOP69lnn73IUQKeIWlAtVq5cqUiIiIUGhqqNm3a6NChQxo3bpzLMX379lVERIRL27t3r3P/okWLdNVVV6lVq1YKDAzUvffee9YfwMDFdvp7/HTr06ePioqKtGTJEg0YMECSNGDAAC1evFjFxcUu5xYXF6tPnz4aN26cJkyYcMb+CwsLq/z76NGjR7XfF3AmvOUS1SolJUUZGRkqKSnRjBkzFBQUpLS0NJdjZsyYoW7durlsi4+Pd349d+5c5w9f6dQP4C5dumjWrFmKjIys3hsAzuP09/hp4eHheuutt9S0aVO1a9dOktS+fXslJCTo7bff1pAhQ5zHhoWF6aabbtKcOXPUt29ftWjRokr/kZGR+vLLL122hYWFVdPdAOdG0oBqFR4ersTEREmnfvm3a9dOmZmZLj84Y2Njncf80fbt27Vp0yb9+9//dvlLrLKyUosWLdKwYcOq9waA8/j99/hpmZmZ+vbbbxUU9NuPWIfDoblz57p87wcGBmr58uXq3bu3UlJS9Mknn1RJHAICAs767wO42BiewEUTEBCgxx57TI8//rhOnDjh1jmZmZnq3Lmzvv76a+Xm5jrbmDFjGKKAT9q6das2b96s7Oxsl+/Z7Oxsbdy4UTt37nQ53mq1aunSpbr22muVkpKi7du311DkwPlRacBFdXr8dvbs2Ro7dqwk6ejRo1XWXoiMjFRISIjeeOMNTZ8+Xa1bt3bZP3ToUL344ov69ttv1apVK0nSL7/8otzcXJfj4uLimGmOiyozM1PXXXedOnfuXGXftddeq8zMzCrrNlitVr377rvq06ePUlJStHbtWuf3tWEYZ1ybpEGDBi5PWQAXA99xuKiCgoI0YsQIPffccyopKZEkDR48WHFxcS5t1qxZeu+993T48GHdddddVfpp0aKFWrRo4VJtWLhwoa6++mqXNmfOnIt2b0B5ebn++c9/Vpm3c1paWpoWLFigioqKKvtCQkK0ZMkS3XDDDUpJSdG2bdskSUVFRVX+fcTFxenQoUPVei/AmfBqbAAA4BYqDQAAwC0kDQAAwC0kDQAAwC0kDQAAwC0kDQAAwC0kDQAAwC0kDQAAwC0kDQAAwC0kDUAtMmjQIPXq1cv5uWvXrho1atRFjyM7O1sWi0VHjx496zEWi0XLly93u8+pU6eqffv2XsX1448/ymKxVFlOHIA5SBoALw0aNEgWi0UWi0UhISFKTEzU9OnTdfLkyWq/9tKlS/Xkk0+6daw7v+gB4Fx4YRVggtTUVM2bN09lZWX64IMPNHz4cAUHB2vSpElVji0vL1dISIgp142OjjalHwBwB5UGwARWq1WxsbFKSEjQQw89pG7duum9996T9NuQwlNPPaX4+Hg1b95ckpSfn6977rlHdevWVXR0tHr27Kkff/zR2WdlZaXGjBmjunXrql69eho/frz++KqYPw5PlJWVacKECbLb7bJarUpMTFRmZqZ+/PFHpaSkSJIuu+wyWSwWDRo0SJLkcDiUnp6uxo0bKywsTO3atdOSJUtcrvPBBx+oWbNmCgsLU0pKikuc7powYYKaNWumOnXqqEmTJpo8efIZX9z02muvyW63q06dOrrnnntUWFjosv/1119XixYtFBoaqqSkJL3yyisexwLgwpA0ANUgLCxM5eXlzs9r1qzRrl27tHr1aq1cuVIVFRXq3r27IiMjtW7dOn322WeKiIhQamqq87x//OMfysrK0ty5c7V+/XodOXJEy5YtO+d177//fr311luaOXOmduzYoddee00RERGy2+169913JUm7du1SQUGBXn75ZUlSenq6FixYoFdffVXffvutRo8erQEDBignJ0fSqeSmd+/euuOOO5Sbm6uhQ4dq4sSJHv83iYyMVFZWlrZv366XX35Zc+bM0YwZM1yOycvL0+LFi7VixQp99NFH+uqrr/Twww8797/55puaMmWKnnrqKe3YsUNPP/20Jk+erPnz53scD4ALYADwysCBA42ePXsahmEYDofDWL16tWG1Wo2xY8c698fExBhlZWXOc9544w2jefPmhsPhcG4rKyszwsLCjFWrVhmGYRhxcXHGc88959xfUVFhNGzY0HktwzCMLl26GI8++qhhGIaxa9cuQ5KxevXqM8b5ySefGJKM//znP85tpaWlRp06dYwNGza4HDtkyBCjb9++hmEYxqRJk4yWLVu67J8wYUKVvv5IkrFs2bKz7n/++eeNDh06OD8/8cQTRmBgoLFv3z7ntg8//NAICAgwCgoKDMMwjKZNmxoLFy506efJJ580kpOTDcMwjD179hiSjK+++uqs1wVw4ZjTAJhg5cqVioiIUEVFhRwOh/r166epU6c697dp08ZlHsPXX3+tvLw8RUZGuvRTWlqq3bt3q7CwUAUFBerYsaNzX1BQkP70pz9VGaI4LTc3V4GBgerSpYvbcefl5en48eO69dZbXbaXl5fr6quvliTt2LHDJQ5JSk5Odvsap7399tuaOXOmdu/ereLiYp08eVI2m83lmEaNGumKK65wuY7D4dCuXbsUGRmp3bt3a8iQIRo2bJjzmJMnTyoqKsrjeAB4jqQBMEFKSooyMjIUEhKi+Ph4BQW5/tMKDw93+VxcXKwOHTrozTffrNLX5ZdffkExhIWFeXxOcXGxJOn99993+WUtnZqnYZaNGzeqf//+mjZtmrp3766oqCgtWrRI//jHPzyOdc6cOVWSmMDAQNNiBXB2JA2ACcLDw5WYmOj28ddcc43efvttNWjQoMpf26fFxcXp888/V+fOnSWd+ot6y5Ytuuaaa854fJs2beRwOJSTk6Nu3bpV2X+60lFZWenc1rJlS1mtVu3du/esFYoWLVo4J3WetmnTpvPf5O9s2LBBCQkJ+tvf/ubc9tNPP1U5bu/evdq/f7/i4+Od1wkICFDz5s0VExOj+Ph4/fDDD+rfv79H1wdgDiZCAjWgf//+ql+/vnr27Kl169Zpz549ys7O1iOPPKJ9+/ZJkh599FE988wzWr58uXbu3KmHH374nGssXHnllRo4cKD+8pe/aPny5c4+Fy9eLElKSEiQxWLRypUr9csvv6i4uFiRkZEaO3asRo8erfnz52v37t368ssvNWvWLOfkwgcffFDff/+9xo0bp127dmnhwoXKysry6H6vuuoq7d27V4sWLdLu3bs1c+bMM07qDA0N1cCBA/X1119r3bp1euSRR3TPPfcoNjZWkjRt2jSlp6dr5syZ+u6777R161bNmzdPL774okfxALgwJA1ADahTp44+/fRTNWrUSL1791aLFi00ZMgQlZaWOisPf/3rX3Xfffdp4MCBSk5OVmRkpO66665z9puRkaG7775bDz/8sJKSkjRs2DCVlJRIkq644gpNmzZNEydOVExMjEaMGCFJevLJJzV58mSlp6erRYsWSk1N1fvvv6/GjRtLOjXP4N1339Xy5cvVrl07vfrqq3r66ac9ut8777xTo0eP1ogRI9S+fXtt2LBBkydPrnJcYmKievfurdtvv1233Xab2rZt6/JI5dChQ/X6669r3rx5atOmjbp06aKsrCxnrACql8U426wqAACA36HSAAAA3ELSAAAA3ELSAAAA3ELSAAAA3ELSAAAA3ELSAAAA3ELSAAAA3ELSAAAA3ELSAAAA3ELSAAAA3ELSAAAA3PL/ATs2tlRHiQ1SAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cfm, display_labels= [\"REAL\", \"FAKE\"])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_16 (Conv3D)          (None, 10, 30, 53, 64)    5248      \n",
      "                                                                 \n",
      " max_pooling3d_20 (MaxPoolin  (None, 5, 15, 27, 64)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 5, 15, 27, 32)     55328     \n",
      "                                                                 \n",
      " max_pooling3d_21 (MaxPoolin  (None, 3, 8, 14, 32)     0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_18 (Conv3D)          (None, 3, 8, 14, 16)      13840     \n",
      "                                                                 \n",
      " max_pooling3d_22 (MaxPoolin  (None, 2, 4, 7, 16)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_19 (Conv3D)          (None, 2, 4, 7, 8)        3464      \n",
      "                                                                 \n",
      " max_pooling3d_23 (MaxPoolin  (None, 1, 2, 4, 8)       0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86,458\n",
      "Trainable params: 86,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3D_col = build_3D_model(vid_arr_col)\n",
    "model_3D_col.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.1)\n",
    "metrics=[\"accuracy\"]\n",
    "model_3D_col.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 - 37s - loss: 3485.9014 - accuracy: 0.6200 - 37s/epoch - 4s/step\n",
      "Epoch 2/10\n",
      "10/10 - 34s - loss: 0.5661 - accuracy: 0.8100 - 34s/epoch - 3s/step\n",
      "Epoch 3/10\n",
      "10/10 - 38s - loss: 0.5054 - accuracy: 0.8100 - 38s/epoch - 4s/step\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_3D_col\u001b[39m.\u001b[39;49mfit(vid_arr_col, y[:vid_arr_col\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]],epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_3D_col.fit(vid_arr_col, y[:vid_arr_col.shape[0]],epochs=10, batch_size=10, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
